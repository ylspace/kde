From patchwork Wed Mar 27 19:29:45 2019
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Sean Christopherson <sean.j.christopherson@intel.com>
X-Patchwork-Id: 10873899
Return-Path: <kvm-owner@kernel.org>
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
 [172.30.200.125])
	by pdx-korg-patchwork-2.web.codeaurora.org (Postfix) with ESMTP id 049D71390
	for <patchwork-kvm@patchwork.kernel.org>;
 Wed, 27 Mar 2019 19:31:53 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E7E5E288B5
	for <patchwork-kvm@patchwork.kernel.org>;
 Wed, 27 Mar 2019 19:31:52 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id DBC0828C7A; Wed, 27 Mar 2019 19:31:52 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.9 required=2.0 tests=BAYES_00,MAILING_LIST_MULTI,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 44553288B5
	for <patchwork-kvm@patchwork.kernel.org>;
 Wed, 27 Mar 2019 19:31:52 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S2387680AbfC0Tbv (ORCPT
        <rfc822;patchwork-kvm@patchwork.kernel.org>);
        Wed, 27 Mar 2019 15:31:51 -0400
Received: from mga11.intel.com ([192.55.52.93]:43953 "EHLO mga11.intel.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1731250AbfC0Tbu (ORCPT <rfc822;kvm@vger.kernel.org>);
        Wed, 27 Mar 2019 15:31:50 -0400
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from fmsmga007.fm.intel.com ([10.253.24.52])
  by fmsmga102.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
 27 Mar 2019 12:31:49 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.60,277,1549958400";
   d="scan'208";a="137931767"
Received: from sjchrist-coffee.jf.intel.com ([10.54.74.181])
  by fmsmga007.fm.intel.com with ESMTP; 27 Mar 2019 12:31:49 -0700
From: Sean Christopherson <sean.j.christopherson@intel.com>
To: Paolo Bonzini <pbonzini@redhat.com>,
 =?utf-8?b?UmFkaW0gS3LEjW3DocWZ?= <rkrcmar@redhat.com>,
 Joerg Roedel <joro@8bytes.org>
Cc: kvm@vger.kernel.org, Jon Doron <arilou@gmail.com>,
        Jim Mattson <jmattson@google.com>,
        Liran Alon <liran.alon@oracle.com>,
        Vitaly Kuznetsov <vkuznets@redhat.com>
Subject: [PATCH 1/2] KVM: x86: Rename pre_{enter,leave}_smm() ops to reference
 SMM state save
Date: Wed, 27 Mar 2019 12:29:45 -0700
Message-Id: <20190327192946.19128-2-sean.j.christopherson@intel.com>
X-Mailer: git-send-email 2.21.0
In-Reply-To: <20190327192946.19128-1-sean.j.christopherson@intel.com>
References: <20190326130746.28748-1-vkuznets@redhat.com>
 <20190327192946.19128-1-sean.j.christopherson@intel.com>
MIME-Version: 1.0
Sender: kvm-owner@vger.kernel.org
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP

Rename pre_{enter,leave}_smm() to post_smi_state_save() and
pre_rsm_load_state() to make it explicitly clear when the callbacks are
invoked, e.g. to allow a future patch to add pre_smi_save_state() and
post_rsm_load_state().

Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
Reviewed-by: Jim Mattson <jmattson@google.com>
---
 arch/x86/include/asm/kvm_emulate.h |  2 +-
 arch/x86/include/asm/kvm_host.h    |  4 ++--
 arch/x86/kvm/emulate.c             |  8 ++++----
 arch/x86/kvm/svm.c                 |  8 ++++----
 arch/x86/kvm/vmx/vmx.c             |  8 ++++----
 arch/x86/kvm/x86.c                 | 14 +++++++-------
 6 files changed, 22 insertions(+), 22 deletions(-)

diff --git a/arch/x86/include/asm/kvm_emulate.h b/arch/x86/include/asm/kvm_emulate.h
index 93c4bf598fb0..b2a65d08d1f8 100644
--- a/arch/x86/include/asm/kvm_emulate.h
+++ b/arch/x86/include/asm/kvm_emulate.h
@@ -226,7 +226,7 @@ struct x86_emulate_ops {
 
 	unsigned (*get_hflags)(struct x86_emulate_ctxt *ctxt);
 	void (*set_hflags)(struct x86_emulate_ctxt *ctxt, unsigned hflags);
-	int (*pre_leave_smm)(struct x86_emulate_ctxt *ctxt, u64 smbase);
+	int (*pre_rsm_load_state)(struct x86_emulate_ctxt *ctxt, u64 smbase);
 
 };
 
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 790876082a77..29ce45f41ee5 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1181,8 +1181,8 @@ struct kvm_x86_ops {
 	void (*get_vmcs12_pages)(struct kvm_vcpu *vcpu);
 
 	int (*smi_allowed)(struct kvm_vcpu *vcpu);
-	int (*pre_enter_smm)(struct kvm_vcpu *vcpu, char *smstate);
-	int (*pre_leave_smm)(struct kvm_vcpu *vcpu, u64 smbase);
+	int (*post_smi_save_state)(struct kvm_vcpu *vcpu, char *smstate);
+	int (*pre_rsm_load_state)(struct kvm_vcpu *vcpu, u64 smbase);
 	int (*enable_smi_window)(struct kvm_vcpu *vcpu);
 
 	int (*mem_enc_op)(struct kvm *kvm, void __user *argp);
diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c
index c338984c850d..ca60eb3358d9 100644
--- a/arch/x86/kvm/emulate.c
+++ b/arch/x86/kvm/emulate.c
@@ -2608,11 +2608,11 @@ static int em_rsm(struct x86_emulate_ctxt *ctxt)
 	smbase = ctxt->ops->get_smbase(ctxt);
 
 	/*
-	 * Give pre_leave_smm() a chance to make ISA-specific changes to the
-	 * vCPU state (e.g. enter guest mode) before loading state from the SMM
-	 * state-save area.
+	 * Give pre_rsm_load_state() a chance to make ISA-specific changes to
+	 * the vCPU state (e.g. enter guest mode) before loading state from the
+	 * SMM state-save area.
 	 */
-	if (ctxt->ops->pre_leave_smm(ctxt, smbase))
+	if (ctxt->ops->pre_rsm_load_state(ctxt, smbase))
 		return X86EMUL_UNHANDLEABLE;
 
 	if (emulator_has_longmode(ctxt))
diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c
index 426039285fd1..c0e511c3b298 100644
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@ -6193,7 +6193,7 @@ static int svm_smi_allowed(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
-static int svm_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
+static int svm_post_smi_save_state(struct kvm_vcpu *vcpu, char *smstate)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
 	int ret;
@@ -6215,7 +6215,7 @@ static int svm_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
 	return 0;
 }
 
-static int svm_pre_leave_smm(struct kvm_vcpu *vcpu, u64 smbase)
+static int svm_pre_rsm_load_state(struct kvm_vcpu *vcpu, u64 smbase)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
 	struct vmcb *nested_vmcb;
@@ -7251,8 +7251,8 @@ static struct kvm_x86_ops svm_x86_ops __ro_after_init = {
 	.setup_mce = svm_setup_mce,
 
 	.smi_allowed = svm_smi_allowed,
-	.pre_enter_smm = svm_pre_enter_smm,
-	.pre_leave_smm = svm_pre_leave_smm,
+	.post_smi_save_state = svm_post_smi_save_state,
+	.pre_rsm_load_state = svm_pre_rsm_load_state,
 	.enable_smi_window = enable_smi_window,
 
 	.mem_enc_op = svm_mem_enc_op,
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 02cf8a551bd1..fc96101e39ac 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -7354,7 +7354,7 @@ static int vmx_smi_allowed(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
-static int vmx_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
+static int vmx_post_smi_save_state(struct kvm_vcpu *vcpu, char *smstate)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 
@@ -7368,7 +7368,7 @@ static int vmx_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
 	return 0;
 }
 
-static int vmx_pre_leave_smm(struct kvm_vcpu *vcpu, u64 smbase)
+static int vmx_pre_rsm_load_state(struct kvm_vcpu *vcpu, u64 smbase)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	int ret;
@@ -7693,8 +7693,8 @@ static struct kvm_x86_ops vmx_x86_ops __ro_after_init = {
 	.setup_mce = vmx_setup_mce,
 
 	.smi_allowed = vmx_smi_allowed,
-	.pre_enter_smm = vmx_pre_enter_smm,
-	.pre_leave_smm = vmx_pre_leave_smm,
+	.post_smi_save_state = vmx_post_smi_save_state,
+	.pre_rsm_load_state = vmx_pre_rsm_load_state,
 	.enable_smi_window = enable_smi_window,
 
 	.check_nested_events = NULL,
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index a419656521b6..abfac99fb7c5 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -5961,9 +5961,9 @@ static void emulator_set_hflags(struct x86_emulate_ctxt *ctxt, unsigned emul_fla
 	kvm_set_hflags(emul_to_vcpu(ctxt), emul_flags);
 }
 
-static int emulator_pre_leave_smm(struct x86_emulate_ctxt *ctxt, u64 smbase)
+static int emulator_pre_rsm_load_state(struct x86_emulate_ctxt *ctxt, u64 smbase)
 {
-	return kvm_x86_ops->pre_leave_smm(emul_to_vcpu(ctxt), smbase);
+	return kvm_x86_ops->pre_rsm_load_state(emul_to_vcpu(ctxt), smbase);
 }
 
 static const struct x86_emulate_ops emulate_ops = {
@@ -6005,7 +6005,7 @@ static const struct x86_emulate_ops emulate_ops = {
 	.set_nmi_mask        = emulator_set_nmi_mask,
 	.get_hflags          = emulator_get_hflags,
 	.set_hflags          = emulator_set_hflags,
-	.pre_leave_smm       = emulator_pre_leave_smm,
+	.pre_rsm_load_state  = emulator_pre_rsm_load_state,
 };
 
 static void toggle_interruptibility(struct kvm_vcpu *vcpu, u32 mask)
@@ -7523,11 +7523,11 @@ static void enter_smm(struct kvm_vcpu *vcpu)
 		enter_smm_save_state_32(vcpu, buf);
 
 	/*
-	 * Give pre_enter_smm() a chance to make ISA-specific changes to the
-	 * vCPU state (e.g. leave guest mode) after we've saved the state into
-	 * the SMM state-save area.
+	 * Give post_smi_save_state() a chance to make ISA-specific changes to
+	 * the vCPU state (e.g. leave guest mode) after we've saved the state
+	 * into the SMM state-save area.
 	 */
-	kvm_x86_ops->pre_enter_smm(vcpu, buf);
+	kvm_x86_ops->post_smi_save_state(vcpu, buf);
 
 	vcpu->arch.hflags |= HF_SMM_MASK;
 	kvm_vcpu_write_guest(vcpu, vcpu->arch.smbase + 0xfe00, buf, sizeof(buf));

From patchwork Wed Mar 27 19:29:46 2019
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Sean Christopherson <sean.j.christopherson@intel.com>
X-Patchwork-Id: 10873901
Return-Path: <kvm-owner@kernel.org>
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
 [172.30.200.125])
	by pdx-korg-patchwork-2.web.codeaurora.org (Postfix) with ESMTP id A3C43186E
	for <patchwork-kvm@patchwork.kernel.org>;
 Wed, 27 Mar 2019 19:31:53 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 92129288B5
	for <patchwork-kvm@patchwork.kernel.org>;
 Wed, 27 Mar 2019 19:31:53 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 85FFF28C75; Wed, 27 Mar 2019 19:31:53 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.9 required=2.0 tests=BAYES_00,MAILING_LIST_MULTI,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id BA30928BD3
	for <patchwork-kvm@patchwork.kernel.org>;
 Wed, 27 Mar 2019 19:31:52 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S2387969AbfC0Tbw (ORCPT
        <rfc822;patchwork-kvm@patchwork.kernel.org>);
        Wed, 27 Mar 2019 15:31:52 -0400
Received: from mga11.intel.com ([192.55.52.93]:43953 "EHLO mga11.intel.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1732964AbfC0Tbv (ORCPT <rfc822;kvm@vger.kernel.org>);
        Wed, 27 Mar 2019 15:31:51 -0400
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from fmsmga007.fm.intel.com ([10.253.24.52])
  by fmsmga102.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
 27 Mar 2019 12:31:50 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.60,277,1549958400";
   d="scan'208";a="137931768"
Received: from sjchrist-coffee.jf.intel.com ([10.54.74.181])
  by fmsmga007.fm.intel.com with ESMTP; 27 Mar 2019 12:31:50 -0700
From: Sean Christopherson <sean.j.christopherson@intel.com>
To: Paolo Bonzini <pbonzini@redhat.com>,
 =?utf-8?b?UmFkaW0gS3LEjW3DocWZ?= <rkrcmar@redhat.com>,
 Joerg Roedel <joro@8bytes.org>
Cc: kvm@vger.kernel.org, Jon Doron <arilou@gmail.com>,
        Jim Mattson <jmattson@google.com>,
        Liran Alon <liran.alon@oracle.com>,
        Vitaly Kuznetsov <vkuznets@redhat.com>
Subject: [PATCH 2/2] KVM: x86: Add kvm_x86_ops callback to allow VMX to stash
 away CR4.VMXE
Date: Wed, 27 Mar 2019 12:29:46 -0700
Message-Id: <20190327192946.19128-3-sean.j.christopherson@intel.com>
X-Mailer: git-send-email 2.21.0
In-Reply-To: <20190327192946.19128-1-sean.j.christopherson@intel.com>
References: <20190326130746.28748-1-vkuznets@redhat.com>
 <20190327192946.19128-1-sean.j.christopherson@intel.com>
MIME-Version: 1.0
Sender: kvm-owner@vger.kernel.org
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP

Intel CPUs manually save and load CR4.VMXE internally instead of having
it automatically switched via the SMM State Save.  Specifically, the
SDM uses the following pseudocode to describe SMI and RSM:

SMI:
  enter SMM;
  save the following internal to the processor:
    CR4.VMXE
    an indication of whether the logical processor was in VMX operation (root or non-root)
  IF the logical processor is in VMX operation THEN
    save current VMCS pointer internal to the processor;
    leave VMX operation;
    save VMX-critical state defined below
  FI;
  CR4.VMXE <- 0;
  perform ordinary SMI delivery:
  save processor state in SMRAM;
  set processor state to standard SMM values;

RSM:
  IF VMXE = 1 in CR4 image in SMRAM THEN
    fail and enter shutdown state;
  ELSE
    restore state normally from SMRAM;
    <unrelated stuff omitted for brevity>
  FI;
  CR4.VMXE <- value stored internally;
  <more stuff omitted for brevity>
  leave SMM;

Presumably, the manual handling of CR4.VMXE is done to avoid having a
to special case CR4.VMXE in the legacy state save/load flows.  As luck
would have it, KVM has a similar conundrum in its SMM state save/load
flows in that vmx_set_cr4() forbids CR4.VMXE from being set while in
SMM.  The check in vmx_set_cr4() can cause RSM to fail when loading
CR4 from the SMM save state area.

Take a page out of the SDM and manually handle CR4.VMXE during SMI and
RSM.  Alternatively, HF_SMM_MASK could be temporarily disabled when
setting CR4 as part of RSM, but doing so is rather ugly due to the need
to plumb the "from_rsm" information through to emulator_set_cr(), and
also opens up a virtualization hole, e.g. KVM would not properly handle
the (extremely unlikely) scenario where the guest toggles CR4.VMXE in
the SMM save state area from 0->1

Reported-by: Jon Doron <arilou@gmail.com>
Suggested-by: Jim Mattson <jmattson@google.com>
Cc: Liran Alon <liran.alon@oracle.com>
Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
Fixes: 5bea5123cbf0 ("KVM: VMX: check nested state and CR4.VMXE against SMM")
Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
---
 arch/x86/include/asm/kvm_emulate.h |  1 +
 arch/x86/include/asm/kvm_host.h    |  2 ++
 arch/x86/kvm/emulate.c             |  2 ++
 arch/x86/kvm/svm.c                 | 12 ++++++++++++
 arch/x86/kvm/vmx/vmx.c             | 22 ++++++++++++++++++++++
 arch/x86/kvm/vmx/vmx.h             |  2 ++
 arch/x86/kvm/x86.c                 |  9 +++++++++
 7 files changed, 50 insertions(+)

diff --git a/arch/x86/include/asm/kvm_emulate.h b/arch/x86/include/asm/kvm_emulate.h
index b2a65d08d1f8..02fa7ec89315 100644
--- a/arch/x86/include/asm/kvm_emulate.h
+++ b/arch/x86/include/asm/kvm_emulate.h
@@ -227,6 +227,7 @@ struct x86_emulate_ops {
 	unsigned (*get_hflags)(struct x86_emulate_ctxt *ctxt);
 	void (*set_hflags)(struct x86_emulate_ctxt *ctxt, unsigned hflags);
 	int (*pre_rsm_load_state)(struct x86_emulate_ctxt *ctxt, u64 smbase);
+	void (*post_rsm_load_state)(struct x86_emulate_ctxt *ctxt);
 
 };
 
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 29ce45f41ee5..0ceb8b353dc0 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1181,8 +1181,10 @@ struct kvm_x86_ops {
 	void (*get_vmcs12_pages)(struct kvm_vcpu *vcpu);
 
 	int (*smi_allowed)(struct kvm_vcpu *vcpu);
+	void (*pre_smi_save_state)(struct kvm_vcpu *vcpu);
 	int (*post_smi_save_state)(struct kvm_vcpu *vcpu, char *smstate);
 	int (*pre_rsm_load_state)(struct kvm_vcpu *vcpu, u64 smbase);
+	void (*post_rsm_load_state)(struct kvm_vcpu *vcpu);
 	int (*enable_smi_window)(struct kvm_vcpu *vcpu);
 
 	int (*mem_enc_op)(struct kvm *kvm, void __user *argp);
diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c
index ca60eb3358d9..706accf524c3 100644
--- a/arch/x86/kvm/emulate.c
+++ b/arch/x86/kvm/emulate.c
@@ -2625,6 +2625,8 @@ static int em_rsm(struct x86_emulate_ctxt *ctxt)
 		return X86EMUL_UNHANDLEABLE;
 	}
 
+	ctxt->ops->post_rsm_load_state(ctxt);
+
 	if ((ctxt->ops->get_hflags(ctxt) & X86EMUL_SMM_INSIDE_NMI_MASK) == 0)
 		ctxt->ops->set_nmi_mask(ctxt, false);
 
diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c
index c0e511c3b298..edc12fb39b32 100644
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@ -6193,6 +6193,11 @@ static int svm_smi_allowed(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+static void svm_pre_smi_save_state(struct kvm_vcpu *vcpu)
+{
+
+}
+
 static int svm_post_smi_save_state(struct kvm_vcpu *vcpu, char *smstate)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -6243,6 +6248,11 @@ static int svm_pre_rsm_load_state(struct kvm_vcpu *vcpu, u64 smbase)
 	return ret;
 }
 
+static void svm_post_rsm_load_state(struct kvm_vcpu *vcpu)
+{
+
+}
+
 static int enable_smi_window(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -7251,8 +7261,10 @@ static struct kvm_x86_ops svm_x86_ops __ro_after_init = {
 	.setup_mce = svm_setup_mce,
 
 	.smi_allowed = svm_smi_allowed,
+	.pre_smi_save_state = svm_pre_smi_save_state,
 	.post_smi_save_state = svm_post_smi_save_state,
 	.pre_rsm_load_state = svm_pre_rsm_load_state,
+	.post_rsm_load_state = svm_post_rsm_load_state,
 	.enable_smi_window = enable_smi_window,
 
 	.mem_enc_op = svm_mem_enc_op,
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index fc96101e39ac..617443c1c6d7 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -7354,6 +7354,16 @@ static int vmx_smi_allowed(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+static void vmx_pre_smi_save_state(struct kvm_vcpu *vcpu)
+{
+	struct vcpu_vmx *vmx = to_vmx(vcpu);
+
+	if (kvm_read_cr4(vcpu) & X86_CR4_VMXE) {
+		vmx->nested.smm.cr4_vmxe = true;
+		WARN_ON(vmx_set_cr4(vcpu, kvm_read_cr4(vcpu) & ~X86_CR4_VMXE));
+	}
+}
+
 static int vmx_post_smi_save_state(struct kvm_vcpu *vcpu, char *smstate)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -7390,6 +7400,16 @@ static int vmx_pre_rsm_load_state(struct kvm_vcpu *vcpu, u64 smbase)
 	return 0;
 }
 
+static void vmx_post_rsm_load_state(struct kvm_vcpu *vcpu)
+{
+	struct vcpu_vmx *vmx = to_vmx(vcpu);
+
+	if (vmx->nested.smm.cr4_vmxe) {
+		WARN_ON(vmx_set_cr4(vcpu, kvm_read_cr4(vcpu) | X86_CR4_VMXE));
+		vmx->nested.smm.cr4_vmxe = false;
+	}
+}
+
 static int enable_smi_window(struct kvm_vcpu *vcpu)
 {
 	return 0;
@@ -7693,8 +7713,10 @@ static struct kvm_x86_ops vmx_x86_ops __ro_after_init = {
 	.setup_mce = vmx_setup_mce,
 
 	.smi_allowed = vmx_smi_allowed,
+	.pre_smi_save_state = vmx_pre_smi_save_state,
 	.post_smi_save_state = vmx_post_smi_save_state,
 	.pre_rsm_load_state = vmx_pre_rsm_load_state,
+	.post_rsm_load_state = vmx_post_rsm_load_state,
 	.enable_smi_window = enable_smi_window,
 
 	.check_nested_events = NULL,
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index a1e00d0a2482..a3444625ca7f 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -162,6 +162,8 @@ struct nested_vmx {
 
 	/* SMM related state */
 	struct {
+		/* CR4.VMXE=1 on SMM entry? */
+		bool cr4_vmxe;
 		/* in VMX operation on SMM entry? */
 		bool vmxon;
 		/* in guest mode on SMM entry? */
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index abfac99fb7c5..1b07706df840 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -5966,6 +5966,11 @@ static int emulator_pre_rsm_load_state(struct x86_emulate_ctxt *ctxt, u64 smbase
 	return kvm_x86_ops->pre_rsm_load_state(emul_to_vcpu(ctxt), smbase);
 }
 
+static void emulator_post_rsm_load_state(struct x86_emulate_ctxt *ctxt)
+{
+	kvm_x86_ops->post_rsm_load_state(emul_to_vcpu(ctxt));
+}
+
 static const struct x86_emulate_ops emulate_ops = {
 	.read_gpr            = emulator_read_gpr,
 	.write_gpr           = emulator_write_gpr,
@@ -6006,6 +6011,7 @@ static const struct x86_emulate_ops emulate_ops = {
 	.get_hflags          = emulator_get_hflags,
 	.set_hflags          = emulator_set_hflags,
 	.pre_rsm_load_state  = emulator_pre_rsm_load_state,
+	.post_rsm_load_state = emulator_post_rsm_load_state,
 };
 
 static void toggle_interruptibility(struct kvm_vcpu *vcpu, u32 mask)
@@ -7516,6 +7522,9 @@ static void enter_smm(struct kvm_vcpu *vcpu)
 	u32 cr0;
 
 	trace_kvm_enter_smm(vcpu->vcpu_id, vcpu->arch.smbase, true);
+
+	kvm_x86_ops->pre_smi_save_state(vcpu);
+
 	memset(buf, 0, 512);
 	if (guest_cpuid_has(vcpu, X86_FEATURE_LM))
 		enter_smm_save_state_64(vcpu, buf);
